---
title: ''
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(car)
library(dplyr)
library(ggpubr)
library(gridExtra)
library(leaps)
library(tidyr)
```

Obtengamos los datos en formato ancho.

```{r}
set.seed(1111)
src_dir <- "~/Downloads"
src_basename <- "EP09 Datos.csv"
src_file <- file.path(src_dir, src_basename)

datos <- read.csv2(file = src_file, stringsAsFactors = TRUE)
```
Generemos las variables nuevas requeridas para este ejercicio.

```{r}
datos_ext <- datos |> 
  mutate(TRG = ifelse(Knees.diameter < 19.0, "no", "s√≠"))
datos_ext[["Gender"]] <- factor(datos_ext[["Gender"]])
datos_ext[["TRG"]] <- factor(datos_ext[["TRG"]])
```

Obtenemos la muestra como indican las instrucciones 1 y 2, teniendo cuidado de desordenar los conjuntos de datos para que no queden juntos todos los casos con la misma clase, puesto que introduce artificialmente dependencia entre los datos.

```{r}
muestra_a <- datos_ext |> filter(Gender == 1 & TRG == "no") |>
  sample_n(75, replace = FALSE)
muestra_b <- datos_ext |> filter(Gender == 1 & TRG == "s√≠") |>
  sample_n(75, replace = FALSE)

i_train <- sample(1:75, 50)
muestra_train <- rbind(muestra_a[i_train, ], muestra_b[i_train, ]) |>
  select(-Gender) |> sample_frac(1L)
muestra_test <- rbind(muestra_a[-i_train, ], muestra_b[-i_train, ]) |>
  select(-Gender) |> sample_frac(1L)
```

Verificamos que no cometimos alg√∫n error con las muestras

```{r}
stopifnot(all(muestra_train$Id == unique(muestra_train$Id)))
stopifnot(all(muestra_test$Id == unique(muestra_test$Id)))
stopifnot(!any(muestra_train$Id %in% muestra_test))
```

Siguiendo la instrucci√≥n 3, recordemos las ocho posibles variables predictoras seleccionadas de forma aleatoria en el ejercicio anterior.

```{r}
nombre_respuesta <- "TRG"
predictores <- c("Ankles.diameter", "Calf.Maximum.Girth", "Waist.Girth", "Bitrochanteric.diameter",
                 "Ankle.Minimum.Girth", "Hip.Girth", "Biiliac.diameter", "Age")
```

#Regresi√≥n log√≠stica simple

Corresponde seleccionar una de las otras variables (instrucci√≥n 4) que podr√≠a ser √∫til para predecir la variable respuesta. Para esto miremos c√≥mo se relacionan las otras variables con la variable de respuesta, sin considerar la variable Gender que, por dise√±o, tiene solo un valor.

```{r}
# Obtiene relaciones entre todos los pares de variables
otras <- colnames(muestra_train)[! colnames(muestra_train) %in% predictores]
p1_dfl <- muestra_train |> select(all_of(otras)) |>
  pivot_longer(-all_of(nombre_respuesta), names_to = "Variable", values_to = "Valor") |>
  mutate(Variable = factor(Variable))
p1 <- ggboxplot(p1_dfl, x = "Variable", y = "Valor", color = nombre_respuesta)
p1 <- p1 +  facet_wrap( ~ Variable, ncol = 4, scales = "free") 
print(p1)
```

Por supuesto, la variable Knees.diameter es la que exhibe menor traslape entre las clases. Es m√°s, no existe traslape para esta variable, por lo que nos permite clasificar los casos sin errores. Como vimos, esto presenta problemas si buscamos un modelo de regresi√≥n log√≠stica, ya que se trata de separaci√≥n perfecta.

```{r}
p2_dfl <- muestra_train |> select(Knees.diameter, TRG) |>
  mutate(Id = 1:n())
p2 <- ggscatter(p2_dfl, x = "Id", y = "Knees.diameter", color = nombre_respuesta)
p2 <- p2 + geom_hline(yintercept = 18.95, linetype = "dashed", color = "green")
p2 <- p2 + theme(axis.title.x = element_blank(), axis.text.x = element_blank(),
                 axis.ticks.x = element_blank())
print(p2)
```

Veamos c√≥mo falla la construcci√≥n del modelo.

```{r}
rlogit_sep_perf <- glm(TRG ~ Knees.diameter, data = muestra_train,
                       family = binomial(link = "logit"))
```

De este modo, tenemos que elegir otra variable para nuestro modelo de regresi√≥n log√≠stica simple (RLogitS). Mirando el gr√°fico de cajas, parece haber varias opciones: Forearm.Girth, Knee.Girth, Shoulder.Girth, Weight, Wrist.Minimum.Girth, y Wrists.diameter parecen tener niveles de solapamiento similares. Pero esta √∫ltima variable parece tener la l√≠neas de las medianas m√°s separadas, por lo que la escogeremos para cumplir con la instrucci√≥n 5.

```{r}
predictor <- "Wrists.diameter"
rlogits_fmla <- formula(paste(nombre_respuesta, predictor, sep = " ~ "))

rlogits <- glm(rlogits_fmla, data = muestra_train,
               family = binomial(link = "logit"))

cat("Modelo de regresi√≥n log√≠stica simple\n")
print(summary(rlogits))
```

# Regresi√≥n log√≠stica m√∫ltiple



Para cumplir con la instrucci√≥n 6, vamos a utilizar el m√©todo de todos los subconjuntos para seleccionar de 2 a 5 predictores para construir un modelo de regresi√≥n log√≠stica m√∫ltiple (RLogitM), teniendo cuidado de forzar a que el predictor usado en el modelo de RLogitS est√© siempre presente.

```{r}
i_respuesta <- match(nombre_respuesta, otras)
i_predictor <- match(predictor, otras)
fuera <- otras[-c(i_respuesta, i_predictor)]
rlogitm_df <- muestra_train |> select(-all_of(fuera))

fmla <- formula(paste(nombre_respuesta, ".", sep = " ~ "))
rlogitm_subsets <- regsubsets(fmla, data = rlogitm_df, nbest = 1, nvmax = 5,
                              force.in = match(predictor, colnames(rlogitm_df)),
                              method = "exhaustive")

plot(rlogitm_subsets)
```

Podemos ver que los modelos m√°s simples son los que producen menores valores de BIC. De acuerdo a las instrucciones del enunciado, deber√≠amos considerar a las variables Ankles.diameter y Calf.Maximum.Girth para ser agregadas al modelo univariado.

```{r}
seleccion <- c("Ankles.diameter", "Calf.Maximum.Girth")
rlogitm_preds_text <- paste(c(predictor, seleccion), collapse = " + ")
rlogitm_fmla <- formula(paste(nombre_respuesta, rlogitm_preds_text, sep = " ~ "))

rlogitm <- glm(rlogitm_fmla, data = muestra_train,
               family = binomial(link = "logit"))

cat("Modelo de regresi√≥n log√≠stica m√∫ltiple\n")
print(summary(rlogitm))
```

# Confiabilidad de los modelos

Ajuste

Comencemos revisando la bondad de ajuste de los modelos.

```{r}
rlogits_lrt <- anova(rlogits, test = "LRT")
rlogitm_lrt <- anova(rlogits, rlogitm, test = "LRT")

cat("Bondad de ajuste del modelo univariado:\n")
print(rlogits_lrt)
cat("\n")
cat("Bondad de ajuste del modelo multivariado:\n")
print(rlogitm_lrt)
```

Vemos que el modelo simple obtiene una reducci√≥n significativa de la devianza (ùúí2(1)=21,145,ùëù<0.001) respecto del modelo nulo, y que el modelo m√∫ltiple logra reducir significativamente este estad√≠stico respecto del modelo simple (ùúí2(2)=17,787,ùëù<0.001). Bajo este criterio entonces, ambos modelos logran una buena bondad de ajsute.

Comprobemos que los residuos (estandarizados) mantienen una media cercana a cero a lo largo de sus gr√°ficos de dispersi√≥n respecto al predictor y a las predicciones que genera. Para esto usaremos la funci√≥n residualPlots() del paquete car.

```{r}
cat("Prueba de curvatura para el predictor del modelo de RLogitS:\n")
residualPlots(rlogits, type = "rstandard", fitted = FALSE,
                                  smooth = list(col="blue"))
```

Vemos que la media, representada por la l√≠nea azul, muestra cierta desviaci√≥n del valor cero, aunque no parece importante ni se ve un patr√≥n distinto evidente. Afortunadamente esta funci√≥n entrega el resultado de una prueba de curvatura, tambi√©n llamada prueba de falta de ajuste, que indica que esta no es significativa, por lo que no hay evidencia suficiente para descartar que los residuos cumplen con la suposici√≥n.

Veamos ahora si esto se verifica para el modelo m√∫ltiple.

```{r}
cat("Prueba de curvatura para los predictores del modelo de RLogitM:\n")
residualPlots(rlogitm, type = "rstandard", smooth = list(col="blue"))
```

De forma similar, las desviaciones de las medias no son significativas, por lo que el ajuste del modelo parece correcto.

# Relaciones lineales
Revisemos ahora el supuesto de linealidad, comenzando con el modelo de RLogitS.

```{r}
rlogits_lin_df <- data.frame(muestra_train[[predictor]],
log(fitted(rlogits)/(1-fitted(rlogits))))
colnames(rlogits_lin_df) <- c(predictor, "Logit")

p_rlogits_lin <- ggscatter(data = rlogits_lin_df, x = predictor, y = "Logit",
                           add = "reg.line", add.params = list(color = "blue"))
print(p_rlogits_lin)
```

Vemos que la relaci√≥n obtenida es perfectamente lineal con el predictor utilizado.

Veamos si este resultado se repite con el modelo de RLogitM.

```{r}
rlogitm_lin_df <- muestra_train[, c(predictor, seleccion)]
rlogitm_lin_df[["Logit"]] <- log(fitted(rlogitm)/(1-fitted(rlogitm)))
colnames(rlogitm_lin_df) <- c(predictor, seleccion, "Logit")

rlogitm_lin_dfl <- pivot_longer(rlogitm_lin_df, -Logit,
                                names_to = "Predictor", values_to = "Valor")

p_rlogitm_lin <- ggscatter(data = rlogitm_lin_dfl, x = "Valor", y = "Logit",
                           add = "reg.line", add.params = list(color = "blue"))
p_rlogitm_lin <- p_rlogitm_lin + facet_wrap(~ Predictor, scales = "free_x")
print(p_rlogitm_lin)
```

¬°Bien! Claramente el modelo logra establecer relaciones lineales con los predictores.

Casos sobre influyentes

Hagamos nuevamente uso de los gr√°ficos de diagn√≥sticos provistos por el paquete car. En este caso, podemos usar la funci√≥n influencePlot() para representar de forma gr√°fica tres m√©tricas de influencia: residuos studentizados versus apalancamiento (hat values) y c√≠rculos cuyas √°reas son proporcionales a la distancia de Cook. Comencemos con el modelo univariado.

```{r}
rlogits_inf_estad <- influencePlot(rlogits, id = list(n = 3))
```

```{r}
cat("Casos notorios para el modelo de RLogitS:\n")
print(rlogits_inf_estad)
```

Adem√°s del gr√°fico, la funci√≥n lista n casos que destacan seg√∫n cada uno de estos tres criterios. En la llamada que hicimos anteriormente solicitamos tres casos para cada uno, pero como algunos se repiten, la funci√≥n devuelve solo cinco casos.

Recordemos los criterios asociados a estas m√©tricas que usamos para identificar casos sospechosos:

    residuos que est√©n m√°s all√° de ¬±2

desviaciones est√°ndares. Para este caso: ùëÑùë°(0,025;ùúà=99)=¬±1,984
.
observaciones con apalancamiento superior al doble (o triple) del apalancamiento promedio. Para este caso: ‚ÑéÃÇ ‚éØ‚éØ‚éØ=2ùëò+1ùëõ=22100=0,4
.
observaciones con distancia de Cook mayor a 0,5

    o muy superior al resto.

Podemos observar que ninguno de los residuos destacados cumple con estas condiciones. Tal vez el caso 1 podr√≠a considerarse algo problem√°tico, pues exhibe una distancia de Cook superior al doble del resto y tiene los valores m√°s altos para los otros criterios (con un empate en apalancamiento con el caso 441). Verifiquemos si el modelo cambia mucho eliminando estas observaciones. Nuevamente el paquete car nos facilita la vida, ahora por medio de la funci√≥n compareCoefs().

```{r}
rlogits_inf_ids <- as.integer(rownames(rlogits_inf_estad))
rlogits_comp_f <- function(s) {
  mat <- eval(bquote(compareCoefs(rlogits, update(rlogits, subset = -.(s)), print = FALSE)))
  rownames(mat) <- paste(rownames(mat), "sin caso", s)
  invisible(mat)
} 
rlogits_comp_list <- lapply(rlogits_inf_ids, rlogits_comp_f) 
rlogits_comp <- do.call(rbind, rlogits_comp_list)

# Agregamos el cambio porcentual
Cambio <- abs((rlogits_comp[, 1]-rlogits_comp[, 3])/rlogits_comp[, 1]) * 100
rlogits_comp <- cbind(rlogits_comp, Cambio)

cat("Comparaci√≥n de modelos de RLogitS con y sin el caso problem√°tico:\n")
printCoefmat(rlogits_comp, digits = 2)
```

Podemos ver que los casos 1 y 46 provocan cambios en los coeficientes alrededor de 1,5. Si bien esto no parece mucho, es mucho mayor a lo que generan las otras observaciones. Por esta raz√≥n, mejor las eliminamos del modelo.

```{r}
rlogits <- update(rlogits, subset = -c(1, 46))

cat("Modelo de regresi√≥n log√≠stica simple actualizado\n")
print(summary(rlogits))
```

Repetimos el proceso con el modelo m√∫ltiple.

```{r}
rlogitm_inf_estad <- influencePlot(rlogitm, id = list(n = 3))
cat("Casos notorios para el modelo de RLogitM:\n")
print(rlogitm_inf_estad)
```

Para este modelo, los valores cr√≠ticos para los residuos studentizados y apalancamiento son ùëÑùë°(0,025;ùúà=97)=¬±1,985 y ‚ÑéÃÇ ‚éØ‚éØ‚éØ=24100=0,08., respectivamente. Vemos que 3 casos con residuos fuera de rango para el 95%

de los datos, lo que es completamente normal. Hay varios casos que sobrepasan el doble del apalancamiento promedio, e incluso el caso 46 sobrepasa el triple de este valor. Ning√∫n caso exhibe distancias de Cook altas, aunque los casos 1, 8 y 231 muestran valores bastante mayores al resto.

Revisemos su influencia en los coeficientes.

```{r}
rlogitm_inf_ids <- as.integer(rownames(rlogitm_inf_estad))
rlogitm_comp_f <- function(s) {
  mat <- eval(bquote(compareCoefs(rlogitm, update(rlogitm, subset = -.(s)), print = FALSE)))
  rownames(mat) <- paste(rownames(mat), "sin caso", s)
  invisible(mat)
} 
rlogitm_comp_list <- lapply(rlogitm_inf_ids, rlogitm_comp_f) 
rlogitm_comp <- do.call(rbind, rlogitm_comp_list)

# Agregamos el cambio porcentual
Cambio <- abs((rlogitm_comp[, 1]-rlogitm_comp[, 3])/rlogitm_comp[, 1]) * 100
rlogitm_comp <- cbind(rlogitm_comp, Cambio)

cat("Comparaci√≥n de modelos de rlogitm con y sin el caso problem√°tico:\n")
printCoefmat(rlogitm_comp, digits = 2)
```

Vemos que eliminar los casos 8 o 58 cambia el coeficiente asociado al predictor ‚ÄòAnkles.diameter‚Äô en m√°s de 3,5%. Eliminando alguno de estos mismos casos o el caso 1 cambia el el coeficiente para el predictor Calf.Maximum.Girth en alrededor de 6,4% para los primeros y ¬°17,1% para el √∫ltimo. Eliminando los casos 46 o 58 cambia el coeficiente del predictor Wrists.diameter en m√°s de 5,1%. Notemos que la presencia de estos casos problem√°ticos pod√≠a deslumbrarse en los gr√°ficos de linealidad que hicimos m√°s arriba. Elimin√©moslos del modelo.

```{r}
rlogitm <- update(rlogitm, subset = -c(1, 8, 46, 58))

cat("Modelo de regresi√≥n log√≠stica m√∫ltiple actualizado\n")
print(summary(rlogitm))
```

# Independencia de los residuos
Confirmemos que no existe dependencia entre los residuos generados por el modelo de RLogitS.
```{r}
cat("Prueba de la independencia de los residuos para el modelo de RLogitS:\n")
print(durbinWatsonTest(rlogits))
```



Vemos que no hay razones para sospechar que los residuos no sean independientes para este modelo.

Confirmemos que esto tambi√©n se da para el modelo de RLogitM.

```{r}
cat("Prueba de la independencia de los residuos para el modelo de RLogitM:\n")
print(durbinWatsonTest(rlogitm))
```

¬°Estupendo! No hay evidencia que nos indique falta de independencia de los residuos en este modelo tampoco.

# Multicolinealidad
Solo nos queda revisar si nuestro modelo m√∫ltiple presenta problemas de multicolinealidad.

```{r}
cat("Factores de inflaci√≥n de la varianza:\n")
print(vif(rlogitm))
cat("\n")
cat("Valores de tolerancia:\n")
print(1 / vif(rlogitm))
```
¬°Fant√°stico! Podemos notar que todos los factores de inflaci√≥n de la varianza est√°n lejos del l√≠mite de 10 y ninguna tolerancia es menos a 0,2, lo que indicar√≠a que no hay presencia de multicolinealidad.

# Resultado

Concluimos que tanto el modelo de RLogitS como el de RLogitM son confiables, luego de eliminar algunos casos sobre influyentes que se hab√≠an considerado en los modelos iniciales.

# Poder predictivo

La instrucci√≥n 8 nos pide evaluar la calidad predictiva de los modelos en t√©rminos de sensibilidad y especificidad (pero sin usar el paquete caret).

Comenzamos obteniendo las predicciones del modelo de RLogitS, tanto en los datos de entrenamiento como en los datos de prueba. Para esto, usaremos el umbral por defecto, y reordenamos las clases para que la clase positiva sea s√≠.

```{r}
umbral <- 0.5

rlogits_probs_train <- fitted(rlogits)
rlogits_preds_train <- sapply(rlogits_probs_train,
function (p) ifelse (p < umbral, "no", "s√≠"))
rlogits_preds_train <- factor(rlogits_preds_train, levels = rev(levels(muestra_train[[nombre_respuesta]])))

rlogits_probs_test <- predict(rlogits, muestra_test, type = "response")
rlogits_preds_test <- sapply(rlogits_probs_test,
function (p) ifelse (p < umbral, "no", "s√≠"))
rlogits_preds_test <- factor(rlogits_preds_test, levels = rev(levels(muestra_train[[nombre_respuesta]])))
```

Teniendo las predicciones, podemos formar las matrices de confusi√≥n y calcular la sensibilidad y especificidad (teniendo cuidado de tambi√©n dar vuelta las clases en los datos observados).

```{r}
rlogits_obs_train <- factor(rlogits[["data"]][names(fitted(rlogits)), nombre_respuesta], levels = rev(levels(muestra_train[[nombre_respuesta]])))
rlogits_obs_test <- factor(muestra_test[[nombre_respuesta]], levels = rev(levels(muestra_train[[nombre_respuesta]])))

rlogits_train_conf_mat <- table(Predicho = rlogits_preds_train, Observado = rlogits_obs_train)
rlogits_test_conf_mat <- table(Predicho = rlogits_preds_test, Observado = rlogits_obs_test)

cat("Matriz de confusi√≥n del modelo de RLogitS en datos de entrenamiento:\n")
print(rlogits_train_conf_mat)
cat("\n")
cat("Matriz de confusi√≥n del modelo de RLogitS en datos de prueba:\n")
print(rlogits_test_conf_mat)
```

Obtengamos la exactitud, sensibilidad y especificidad en cada caso y comparemos sus diferencias al pasar de datos vistos por el modelo a no vistos.

```{r}
rlogits_train_exa <- (rlogits_train_conf_mat[1, 1] + rlogits_train_conf_mat[2, 2]) /
sum(rlogits_train_conf_mat)
rlogits_train_sen <- rlogits_train_conf_mat[1, 1] /
sum(rlogits_train_conf_mat[, 1])
rlogits_train_esp <- rlogits_train_conf_mat[2, 2] /
sum(rlogits_train_conf_mat[, 2])

rlogits_test_exa <- (rlogits_test_conf_mat[1, 1] + rlogits_test_conf_mat[2, 2]) /
sum(rlogits_test_conf_mat)
rlogits_test_sen <- rlogits_test_conf_mat[1, 1] /
sum(rlogits_test_conf_mat[, 1])
rlogits_test_esp <- rlogits_test_conf_mat[2, 2] /
sum(rlogits_test_conf_mat[, 2])

rlogits_cambio_exa <- (rlogits_train_exa - rlogits_test_exa) / rlogits_test_exa * 100
rlogits_cambio_sen <- (rlogits_train_sen - rlogits_test_sen) / rlogits_test_sen * 100
rlogits_cambio_esp <- (rlogits_train_esp - rlogits_test_esp) / rlogits_test_esp * 100

cat("Rendimiento del modelo de RLogitS en datos de entrenamiento:\n")
cat(sprintf("    Exactitud: %.2f\n", rlogits_train_exa))
cat(sprintf(" Sensibilidad: %.2f\n", rlogits_train_sen))
cat(sprintf("Especificidad: %.2f\n", rlogits_train_esp))
cat("\n")
cat("Rendimiento del modelo de RLogitS en datos de prueba:\n")
cat(sprintf("    Exactitud: %.2f\n", rlogits_test_exa))
cat(sprintf(" Sensibilidad: %.2f\n", rlogits_test_sen))
cat(sprintf("Especificidad: %.2f\n", rlogits_test_esp))
cat("\n")
cat("Cambio porcentual en el rendimiento del modelo de RLogitS:\n")
cat(sprintf("    Exactitud: %7.2f%%\n", rlogits_cambio_exa))
cat(sprintf(" Sensibilidad: %7.2f%%\n", rlogits_cambio_sen))
cat(sprintf("Especificidad: %7.2f%%\n", rlogits_cambio_esp))
```
Vemos que la exactitud no sufre un cambio importante, pero s√≠ se observa un aumnento en la sensibilidad y una ca√≠da de la especificidad. En general, parece que el modelo se comporta bien con datos no vistos.

Repitamos el an√°lisis con el modelo m√∫ltiple.

```{r}
rlogitm_probs_train <- fitted(rlogitm)
rlogitm_preds_train <- sapply(rlogitm_probs_train,
function (p) ifelse (p < umbral, "no", "s√≠"))
rlogitm_preds_train <- factor(rlogitm_preds_train, levels = rev(levels(muestra_train[[nombre_respuesta]])))

rlogitm_probs_test <- predict(rlogitm, muestra_test, type = "response")
rlogitm_preds_test <- sapply(rlogitm_probs_test,
function (p) ifelse (p < umbral, "no", "s√≠"))
rlogitm_preds_test <- factor(rlogitm_preds_test, levels = rev(levels(muestra_train[[nombre_respuesta]])))

rlogitm_obs_train <- factor(rlogitm[["data"]][names(fitted(rlogitm)), nombre_respuesta],
                            levels = rev(levels(muestra_train[[nombre_respuesta]])))
rlogitm_obs_test <- factor(muestra_test[[nombre_respuesta]],
                           levels = rev(levels(muestra_train[[nombre_respuesta]])))

rlogitm_train_conf_mat <- table(Predicho = rlogitm_preds_train, Observado = rlogitm_obs_train)
rlogitm_test_conf_mat <- table(Predicho = rlogitm_preds_test, Observado = rlogitm_obs_test)

cat("Matriz de confusi√≥n del modelo de RLogitM en datos de entrenamiento:\n")
print(rlogitm_train_conf_mat)
cat("\n")
cat("Matriz de confusi√≥n del modelo de RLogitM en datos de prueba:\n")
print(rlogitm_test_conf_mat)
```

Obtengamos las m√©tricas de desempe√±o y compar√©moslas al pasar de datos vistos a los no vistos.

```{r}
rlogitm_train_exa <- (rlogitm_train_conf_mat[1, 1] + rlogitm_train_conf_mat[2, 2]) /
sum(rlogitm_train_conf_mat)
rlogitm_train_sen <- rlogitm_train_conf_mat[1, 1] /
sum(rlogitm_train_conf_mat[, 1])
rlogitm_train_esp <- rlogitm_train_conf_mat[2, 2] /
sum(rlogitm_train_conf_mat[, 2])

rlogitm_test_exa <- (rlogitm_test_conf_mat[1, 1] + rlogitm_test_conf_mat[2, 2]) /
sum(rlogitm_test_conf_mat)
rlogitm_test_sen <- rlogitm_test_conf_mat[1, 1] /
sum(rlogitm_test_conf_mat[, 1])
rlogitm_test_esp <- rlogitm_test_conf_mat[2, 2] /
sum(rlogitm_test_conf_mat[, 2])

rlogitm_cambio_exa <- (rlogitm_train_exa - rlogitm_test_exa) / rlogitm_test_exa * 100
rlogitm_cambio_sen <- (rlogitm_train_sen - rlogitm_test_sen) / rlogitm_test_sen * 100
rlogitm_cambio_esp <- (rlogitm_train_esp - rlogitm_test_esp) / rlogitm_test_esp * 100

cat("Rendimiento del modelo de RLogitM en datos de entrenamiento:\n")
cat(sprintf("    Exactitud: %.2f\n", rlogitm_train_exa))
cat(sprintf(" Sensibilidad: %.2f\n", rlogitm_train_sen))
cat(sprintf("Especificidad: %.2f\n", rlogitm_train_esp))
cat("\n")
cat("Rendimiento del modelo de RLogitM en datos de prueba:\n")
cat(sprintf("    Exactitud: %.2f\n", rlogitm_test_exa))
cat(sprintf(" Sensibilidad: %.2f\n", rlogitm_test_sen))
cat(sprintf("Especificidad: %.2f\n", rlogitm_test_esp))
cat("\n")
cat("Cambio porcentual en el rendimiento del modelo de RLogitM:\n")
cat(sprintf("    Exactitud: %7.2f%%\n", rlogitm_cambio_exa))
cat(sprintf(" Sensibilidad: %7.2f%%\n", rlogitm_cambio_sen))
cat(sprintf("Especificidad: %7.2f%%\n", rlogitm_cambio_esp))
```

¬°Oh! Aqu√≠ s√≠ hay una ca√≠da notoria de todas las m√©tricas de desempe√±o cuando el modelo hace predicciones con datos no vistos.

# Resultado

Ambos modelos muestran un calidad predictiva moderada, con una sensibilidad sobre 70%
y una especificidad sobre 60% en datos no utilizados para construirlos.

El modelo simple muestra cierta estabilidad en el rendimiento al pasar de datos conocidos a desconocidos. Sin embargo, el modelo de RLogM parece tener problemas de generalizaci√≥n puesto que presenta una ca√≠da importante en el rendimiento al ser aplicado a datos no conocidos. Esto es una indicaci√≥n de sobreajuste y habr√≠a que explorar la eliminaci√≥n de alg√∫n predictor, aunque eso nos har√≠a incumplir con lo solicitado en el enunciado.
